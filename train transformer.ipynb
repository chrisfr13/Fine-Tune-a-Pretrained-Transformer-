{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11173423,"sourceType":"datasetVersion","datasetId":6973376}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ------------------ 1. Εισαγωγή βιβλιοθηκών ------------------\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nfrom transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\n\n# Απενεργοποίηση wandb\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Έλεγχος για GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------ 2. Φόρτωση και επεξεργασία δεδομένων ------------------\ndf = pd.read_csv(\"/kaggle/input/tweets/Tweets.csv\")\ndf = df[['text', 'airline_sentiment']]\nlabel2id = {'negative': 0, 'neutral': 1, 'positive': 2}\nid2label = {0: 'negative', 1: 'neutral', 2: 'positive'}\ndf['label'] = df['airline_sentiment'].map(label2id)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------ 3. Split σε train / val / test ------------------\ntrain_texts, temp_texts, train_labels, temp_labels = train_test_split(\n    df['text'].tolist(), df['label'].tolist(), test_size=0.3, random_state=42)\n\nval_texts, test_texts, val_labels, test_labels = train_test_split(\n    temp_texts, temp_labels, test_size=0.5, random_state=42)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------ 4. Tokenization ------------------\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------ 5. Dataset Class ------------------\nclass TweetDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = TweetDataset(train_encodings, train_labels)\nval_dataset = TweetDataset(val_encodings, val_labels)\ntest_dataset = TweetDataset(test_encodings, test_labels)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------ 6. Φόρτωση Μοντέλου ------------------\nmodel = DistilBertForSequenceClassification.from_pretrained(\n    'distilbert-base-uncased',\n    num_labels=3,\n    id2label=id2label,\n    label2id=label2id\n)\nmodel.to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------ 7. Phase 1: Πάγωμα Backbone ------------------\nfor param in model.distilbert.parameters():\n    param.requires_grad = False\n\ntraining_args_1 = TrainingArguments(\n    output_dir='./results_phase1',\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    learning_rate=5e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n)\n\ntrainer_1 = Trainer(\n    model=model,\n    args=training_args_1,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n)\n\nprint(\"===== Phase 1: Training classification head only =====\")\ntrainer_1.train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------ 8. Phase 2: Ξεπαγώνουμε όλα τα layers ------------------\nfor param in model.distilbert.parameters():\n    param.requires_grad = True\n\ntraining_args_2 = TrainingArguments(\n    output_dir='./results_phase2',\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    logging_dir='./logs_phase2',\n    logging_steps=10,\n)\n\ntrainer_2 = Trainer(\n    model=model,\n    args=training_args_2,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n)\n\nprint(\"===== Phase 2: Fine-tuning all layers =====\")\ntrainer_2.train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------ 9. Αξιολόγηση στο test set ------------------\npreds = trainer_2.predict(test_dataset)\ny_pred = np.argmax(preds.predictions, axis=1)\n\ncm = confusion_matrix(test_labels, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', xticklabels=id2label.values(), yticklabels=id2label.values())\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n\nprint(classification_report(test_labels, y_pred, target_names=id2label.values()))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------ 10. Καμπύλες εκπαίδευσης ------------------\nlogs = trainer_2.state.log_history\ntrain_loss = [x['loss'] for x in logs if 'loss' in x]\neval_loss = [x['eval_loss'] for x in logs if 'eval_loss' in x]\n\nplt.plot(train_loss, label='Train Loss')\nplt.plot(eval_loss, label='Validation Loss')\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Training vs Validation Loss')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Το μοντέλο που επιλέξαμε είναι το *DistilBERT*, ένα ελαφρύτερο και ταχύτερο παράγωγο του BERT. Διαθέτει 6 layers (αντί για 12) και προσφέρει πολύ καλή απόδοση με μικρότερο κόστος υπολογισμού.\n\nΕφαρμόσαμε gradual unfreezing:\n- Στην *1η φάση*, εκπαιδεύσαμε μόνο τον ταξινομητή κρατώντας το υπόλοιπο μοντέλο \"παγωμένο\".\n- Στη *2η φάση*, ξεπαγώσαμε όλα τα layers και κάναμε πλήρες fine-tuning.\n\nΑυτό βοηθά να διατηρηθούν οι προεκπαιδευμένες γνώσεις και να προσαρμοστεί καλύτερα το μοντέλο στα νέα δεδομένα, αποφεύγοντας το overfitting.\n","metadata":{}}]}